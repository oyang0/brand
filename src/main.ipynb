{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2741a5b6-28df-4fe4-87cd-11f3ddd79c34",
   "metadata": {},
   "source": [
    "# Brand Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfe05d2-2940-44ad-808f-c72b3d637a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from re import search\n",
    "from random import seed\n",
    "from numpy.random import seed as np_seed\n",
    "from eda import stop_words, get_only_chars, eda\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a00bdf-2052-4d9a-b83f-1fcfe27455e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a3d550-d69b-49ce-9c81-ddf6aca08366",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "bos_token = \"[BOS]\"\n",
    "eos_token = \"[EOS]\"\n",
    "pad_token = \"[PAD]\"\n",
    "sep_token = \"[SEP]\"\n",
    "\n",
    "\n",
    "class TransformersDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_mask):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.input_ids.iloc[idx]\n",
    "        attention_mask = self.attention_mask.iloc[idx]\n",
    "        return input_ids, attention_mask\n",
    "\n",
    "\n",
    "def train(dataloader, model, optimizer, device=\"cuda\", step_size=1, verbose=False):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    steps = 0\n",
    "    train_loss = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X, attention_mask=y, labels=X)\n",
    "        loss = pred.loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        steps += 1\n",
    "        if steps % step_size == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            if verbose:\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    optimizer.step()\n",
    "    train_loss /= num_batches\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def test(dataloader, model, device=\"cuda\", verbose=False):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X, attention_mask=y, labels=X)\n",
    "            test_loss += pred.loss.item()\n",
    "    test_loss /= num_batches\n",
    "    if verbose:\n",
    "        print(f\"Test Error: \\n Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss\n",
    "\n",
    "\n",
    "def learn(\n",
    "    training_data,\n",
    "    test_data,\n",
    "    model,\n",
    "    optimizer,\n",
    "    batch_size=64,\n",
    "    device=\"cuda\",\n",
    "    epochs=5,\n",
    "    step_size=1,\n",
    "    file=None,\n",
    "    verbose=False,\n",
    "):\n",
    "    # Create data loader.\n",
    "    train_dataloader = DataLoader(training_data, batch_size)\n",
    "    test_dataloader = DataLoader(test_data, batch_size)\n",
    "\n",
    "    for X, y in test_dataloader:\n",
    "        if verbose:\n",
    "            print(\"Shape of X: \", X.shape)\n",
    "            print(\"Shape of y: \", y.shape, y.dtype)\n",
    "            print(f\"Using {device} device\")\n",
    "            print(model)\n",
    "        break\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    for t in range(epochs):\n",
    "        if verbose:\n",
    "            print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loss = train(\n",
    "            train_dataloader, model, optimizer, device, step_size, verbose\n",
    "        )\n",
    "        test_loss = test(test_dataloader, model, device, verbose)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        if file:\n",
    "            torch.save(model, f\"{file}-{t+1}.pth\")\n",
    "            if verbose:\n",
    "                print(f\"Saved PyTorch Model State to {file}-{t+1}.pth\")\n",
    "    if verbose:\n",
    "        print(\"Done!\")\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "def generate(\n",
    "    test_data,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    batch_size=64,\n",
    "    device=\"cuda\",\n",
    "    pat=\"(.*)\",\n",
    "    flags=0,\n",
    "    verbose=False,\n",
    "):\n",
    "    # Create data loader.\n",
    "    test_dataloader = DataLoader(test_data, batch_size)\n",
    "\n",
    "    for X, y in test_dataloader:\n",
    "        if verbose:\n",
    "            print(\"Shape of X: \", X.shape)\n",
    "            print(\"Shape of y: \", y.shape, y.dtype)\n",
    "            print(f\"Using {device} device\")\n",
    "            print(model)\n",
    "        break\n",
    "\n",
    "    pad_token_id = tokenizer(pad_token)[\"input_ids\"][0]\n",
    "    eos_token_id = tokenizer(eos_token)[\"input_ids\"][0]\n",
    "    generated = pd.Series(dtype=str)\n",
    "\n",
    "    for X, y in test_dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        outputs = model.generate(\n",
    "            X,\n",
    "            attention_mask=y,\n",
    "            max_length=50,\n",
    "            # num_beams=5,\n",
    "            # early_stopping=True,\n",
    "            pad_token_id=pad_token_id,\n",
    "            eos_token_id=eos_token_id,\n",
    "        )\n",
    "        outputs = pd.Series(tokenizer.batch_decode(outputs)) + eos_token\n",
    "        outputs = outputs.str.extract(pat, flags, False)\n",
    "        generated = pd.concat([generated, outputs], ignore_index=True)\n",
    "\n",
    "    return generated\n",
    "\n",
    "\n",
    "def get_document(df, col1, col2):\n",
    "    return get_input_context(df, col1) + df[col2] + eos_token\n",
    "\n",
    "\n",
    "def get_input_context(df, col):\n",
    "    return bos_token + df[col] + sep_token\n",
    "\n",
    "\n",
    "def get_augmented_sentences(\n",
    "    df,\n",
    "    col1,\n",
    "    col2,\n",
    "    alpha_sr=0.1,\n",
    "    alpha_ri=0.1,\n",
    "    alpha_rs=0.1,\n",
    "    p_rd=0.1,\n",
    "    num_aug=9,\n",
    "):\n",
    "    def augment(val1, val2):\n",
    "        return pd.Series(\n",
    "            eda(\n",
    "                val2,\n",
    "                alpha_sr,\n",
    "                alpha_ri,\n",
    "                alpha_rs,\n",
    "                p_rd,\n",
    "                num_aug,\n",
    "                stop_words + val1.split(),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    augmented_sentences = df.apply(lambda x: augment(x[col1], x[col2]), axis=1)\n",
    "    augmented_sentences = augmented_sentences.transpose()\n",
    "    augmented_sentences = augmented_sentences.rename(columns=df[col1].to_dict())\n",
    "    augmented_sentences = augmented_sentences.melt(var_name=col1, value_name=col2)\n",
    "    return augmented_sentences\n",
    "\n",
    "\n",
    "def get_augmented_labels(df, col1, col2, num_aug=9):\n",
    "    def augment(val1, val2):\n",
    "        if isin(val2, val1):\n",
    "            val3 = f\" {df[col1].sample().iloc[0]} \"\n",
    "            val2 = f\" {val2} \".replace(f\" {val1} \", val3).strip()\n",
    "            val1 = val3.strip()\n",
    "        return pd.Series([val1, val2], index=[col1, col2])\n",
    "\n",
    "    augmented_labels = pd.concat(\n",
    "        [df.apply(lambda x: augment(x[col1], x[col2]), axis=1) for _ in range(num_aug)],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    return augmented_labels\n",
    "\n",
    "\n",
    "def isin(sentence, value):\n",
    "    sentence = f\" {sentence} \"\n",
    "    value = f\" {value} \"\n",
    "    return value in sentence\n",
    "\n",
    "\n",
    "def startswith(sentence, value):\n",
    "    sentence = f\"{sentence} \"\n",
    "    value = f\"{value} \"\n",
    "    return sentence.startswith(value)\n",
    "\n",
    "\n",
    "def get_model(tokenizer, model, device=\"cuda\"):\n",
    "    def add_special_tokens(tokenizer):\n",
    "        tokenizer.add_special_tokens(\n",
    "            {\n",
    "                \"bos_token\": bos_token,\n",
    "                \"eos_token\": eos_token,\n",
    "                \"sep_token\": sep_token,\n",
    "                \"pad_token\": pad_token,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    tokenizer = torch.load(tokenizer)\n",
    "    add_special_tokens(tokenizer)\n",
    "\n",
    "    model = torch.load(model)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model = model.to(device)\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "def get_dataset(tokenizer, ser):\n",
    "    def to_tensors(seq):\n",
    "        return pd.Series([torch.tensor(obj) for obj in seq])\n",
    "\n",
    "    encoded_input = tokenizer(ser.tolist(), padding=True)\n",
    "    input_ids = to_tensors(encoded_input[\"input_ids\"])\n",
    "    attention_mask = to_tensors(encoded_input[\"attention_mask\"])\n",
    "    dataset = TransformersDataset(input_ids, attention_mask)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def delete_special_tokens(ser):\n",
    "    ser = ser.str.replace(bos_token, \"\", regex=False)\n",
    "    ser = ser.str.replace(eos_token, \"\", regex=False)\n",
    "    ser = ser.str.replace(sep_token, \"\", regex=False)\n",
    "    ser = ser.str.replace(pad_token, \"\", regex=False)\n",
    "    return ser\n",
    "\n",
    "\n",
    "def get_accuracy(ser1, ser2, verbose=False):\n",
    "    ser1 = ser1.reset_index(drop=True)\n",
    "    ser2 = ser2.reset_index(drop=True)\n",
    "    size = len(ser1)\n",
    "    correct = (ser1 == ser2).sum()\n",
    "    correct /= size\n",
    "    if verbose:\n",
    "        print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}% \\n\")\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10623592-d752-4b36-b52d-b37a75b6b435",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Obtaining Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39f72ac-2013-4490-9607-abca7b5d75ae",
   "metadata": {},
   "source": [
    "Hackathon_Ideal_Data.csv is available at [Store Transaction data](https://www.kaggle.com/iamprateek/store-transaction-data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930215bc-0599-430b-b1b4-b14d630332f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"../data/Hackathon_Ideal_Data.csv\")\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cc6c2c-b0cd-4e88-a5be-7fb9cf9e37dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scrubbing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5931f2-95ca-47b3-9dd4-69e5c52db904",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1[[\"MBRD\", \"BRD\"]]\n",
    "df2 = df2.rename(columns={\"MBRD\": \"brand\", \"BRD\": \"product\"})\n",
    "df2[\"brand\"] = df2[\"brand\"].apply(get_only_chars)\n",
    "df2[\"brand\"] = df2[\"brand\"].str.strip()\n",
    "df2 = df2[df2[\"brand\"].str.len() > 0]\n",
    "df2[\"product\"] = df2[\"product\"].apply(get_only_chars)\n",
    "df2[\"product\"] = df2[\"product\"].str.strip()\n",
    "df2 = df2[df2[\"product\"].str.len() > 0]\n",
    "df2[\"document\"] = get_document(df2, \"product\", \"brand\")\n",
    "df2[\"input context\"] = get_input_context(df2, \"product\")\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31df32f7-394e-4611-b99e-e055dc32656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set1 = df2.sample(frac=0.7, random_state=1)\n",
    "training_set1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9165f4a9-a249-4696-8500-8dee0d2146d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set1 = df2.drop(training_set1.index)\n",
    "validation_set1 = validation_set1.sample(frac=0.7, random_state=1)\n",
    "validation_set1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3184c96-559a-4469-9645-4739f3088022",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set1 = df2.drop(training_set1.index)\n",
    "test_set1 = test_set1.drop(validation_set1.index)\n",
    "test_set1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cff0a2-f44f-4955-a135-b41c5b800f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1)\n",
    "augmented_sentences = get_augmented_sentences(\n",
    "    training_set1,\n",
    "    \"brand\",\n",
    "    \"product\",\n",
    "    alpha_sr=0.1,\n",
    "    alpha_ri=0.1,\n",
    "    alpha_rs=0.1,\n",
    "    p_rd=0.1,\n",
    "    num_aug=4,\n",
    ")\n",
    "training_set2 = pd.concat([training_set1, augmented_sentences])\n",
    "training_set2[\"document\"] = get_document(training_set2, \"product\", \"brand\")\n",
    "training_set2[\"input context\"] = get_input_context(training_set2, \"product\")\n",
    "training_set2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bd1aae-e216-42f2-834b-0ed95048fc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_seed(1)\n",
    "augmented_labels = get_augmented_labels(training_set1, \"brand\", \"product\", num_aug=4)\n",
    "training_set3 = pd.concat([training_set1, augmented_labels])\n",
    "training_set3[\"document\"] = get_document(training_set3, \"product\", \"brand\")\n",
    "training_set3[\"input context\"] = get_input_context(training_set3, \"product\")\n",
    "training_set3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3454c9b3-a58c-4612-a7fd-407bca296a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set4 = pd.concat([training_set2, augmented_labels])\n",
    "training_set4[\"document\"] = get_document(training_set4, \"product\", \"brand\")\n",
    "training_set4[\"input context\"] = get_input_context(training_set4, \"product\")\n",
    "training_set4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7dcd88-dc9f-4485-a789-bb5b970eec5a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exploring Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f461b01e-2636-4fbe-a6df-cde5f8cafe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set1[\"brand\"].str.split(\" \").str.len().value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40a11fb-0850-4a6a-a1ac-127d6537074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set1.apply(\n",
    "    lambda x: isin(x[\"product\"], x[\"brand\"]), axis=1\n",
    ").value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c807ac-7cde-4b58-a378-03ee7582842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set1.apply(\n",
    "    lambda x: startswith(x[\"product\"], x[\"brand\"]), axis=1\n",
    ").value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3139e7a-5fb7-49b9-a934-05b625348e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set1.apply(\n",
    "    lambda x: isin(x[\"product\"], x[\"brand\"])\n",
    "    and not startswith(x[\"product\"], x[\"brand\"]),\n",
    "    axis=1,\n",
    ").value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce985a18-3786-416c-ba6c-0a19edc6ce75",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Modelling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c19a15-513a-4cc0-bf7c-22fc4c4475c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = get_model(\"../models/gpt2-tokenizer.pth\", \"../models/gpt2-model.pth\", device)\n",
    "training_data1 = get_dataset(tokenizer, training_set1[\"document\"])\n",
    "training_data2 = get_dataset(tokenizer, training_set2[\"document\"])\n",
    "training_data3 = get_dataset(tokenizer, training_set3[\"document\"])\n",
    "training_data4 = get_dataset(tokenizer, training_set4[\"document\"])\n",
    "validation_data1 = get_dataset(tokenizer, validation_set1[\"document\"])\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "train_losses, test_losses = learn(\n",
    "    training_data2,\n",
    "    validation_data1,\n",
    "    model,\n",
    "    optimizer,\n",
    "    batch_size=32,\n",
    "    device=device,\n",
    "    epochs=5,\n",
    "    step_size=2,\n",
    "    file=\"gpt2\",\n",
    "    verbose=verbose,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf05868a-016d-4ed4-9e3f-7ca77b9a0a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = get_model(\"../models/gpt2-tokenizer.pth\", \"../models/gpt2-5.pth\", device)\n",
    "validation_data1 = get_dataset(tokenizer, validation_set1[\"input context\"])\n",
    "sep_token_re = sep_token.replace(\"[\", \"\\[\").replace(\"]\", \"\\]\")\n",
    "eos_token_re = eos_token.replace(\"[\", \"\\[\").replace(\"]\", \"\\]\")\n",
    "pat = f\"^.*?{sep_token_re}\\s*(.*?)\\s*{eos_token_re}.*$\"\n",
    "\n",
    "outputs = generate(\n",
    "    validation_data1,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    batch_size=32,\n",
    "    device=device,\n",
    "    pat=pat,\n",
    "    flags=0,\n",
    "    verbose=False,\n",
    ")\n",
    "outputs = delete_special_tokens(outputs)\n",
    "outputs = outputs.replace(\"\", \" \")\n",
    "outputs = outputs.apply(get_only_chars)\n",
    "outputs = outputs.str.strip()\n",
    "\n",
    "accuracy = get_accuracy(outputs, validation_set1[\"brand\"], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e573ad-8073-49ae-bfb6-01ac9c333222",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Interpreting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88fbed2-c3fd-4f05-a405-2db27000c471",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = get_model(\"../models/gpt2-tokenizer.pth\", \"../models/gpt2-5.pth\", device)\n",
    "validation_data1 = get_dataset(tokenizer, test_set1[\"input context\"])\n",
    "sep_token_re = sep_token.replace(\"[\", \"\\[\").replace(\"]\", \"\\]\")\n",
    "eos_token_re = eos_token.replace(\"[\", \"\\[\").replace(\"]\", \"\\]\")\n",
    "pat = f\"^.*?{sep_token_re}\\s*(.*?)\\s*{eos_token_re}.*$\"\n",
    "\n",
    "outputs = generate(\n",
    "    validation_data1,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    batch_size=32,\n",
    "    device=device,\n",
    "    pat=pat,\n",
    "    flags=0,\n",
    "    verbose=False,\n",
    ")\n",
    "outputs = delete_special_tokens(outputs)\n",
    "outputs = outputs.replace(\"\", \" \")\n",
    "outputs = outputs.apply(get_only_chars)\n",
    "outputs = outputs.str.strip()\n",
    "\n",
    "accuracy = get_accuracy(outputs, test_set1[\"brand\"], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ee7943-3b35-4114-901a-be66f2c6f8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = test_set1[\"product\"].str.extract(\"^([^ ]*) ?.*$\", expand=False)\n",
    "accuracy = get_accuracy(outputs, test_set1[\"brand\"], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b7c2cf-0437-4a5f-beca-b23626676592",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
