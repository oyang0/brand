{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2741a5b6-28df-4fe4-87cd-11f3ddd79c34",
   "metadata": {},
   "source": [
    "# Brand Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfe05d2-2940-44ad-808f-c72b3d637a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from re import search\n",
    "from random import seed\n",
    "from numpy.random import seed as np_seed\n",
    "from time import perf_counter\n",
    "from eda import stop_words, get_only_chars, eda\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a00bdf-2052-4d9a-b83f-1fcfe27455e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a3d550-d69b-49ce-9c81-ddf6aca08366",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "class SeqToSeqDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_mask, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.input_ids[idx]\n",
    "        attention_mask = self.attention_mask[idx]\n",
    "        labels = self.labels[idx]\n",
    "        return input_ids, attention_mask, labels\n",
    "\n",
    "\n",
    "def train(dataloader, model, optimizer, device=\"cuda\", step_size=1, verbose=False):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    steps = 0\n",
    "    train_loss = 0\n",
    "    for batch, (input_ids, attention_mask, labels) in enumerate(dataloader):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = pred.loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        steps += 1\n",
    "        if steps % step_size == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0 and verbose:\n",
    "            loss, current = loss.item(), batch * len(input_ids)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    optimizer.step()\n",
    "    train_loss /= num_batches\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def test(dataloader, model, device=\"cuda\", verbose=False):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, labels in dataloader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "            pred = model(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, labels=labels\n",
    "            )\n",
    "            test_loss += pred.loss.item()\n",
    "            output_sequences = model.generate(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, do_sample=False\n",
    "            )\n",
    "            for output_sequence, label in zip(output_sequences, labels):\n",
    "                output_sequence = output_sequence[output_sequence != 0]\n",
    "                label = label[label != -100]\n",
    "                if output_sequence.shape == label.shape:\n",
    "                    if all(output_sequence == label):\n",
    "                        correct += 1\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\"\n",
    "        )\n",
    "    return test_loss, correct\n",
    "\n",
    "\n",
    "def learn(\n",
    "    training_data,\n",
    "    test_data,\n",
    "    model,\n",
    "    optimizer,\n",
    "    batch_size=64,\n",
    "    device=\"cuda\",\n",
    "    epochs=5,\n",
    "    step_size=1,\n",
    "    file=None,\n",
    "    verbose=False,\n",
    "):\n",
    "    # Create data loader.\n",
    "    train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "    for input_ids, attention_mask, labels in test_dataloader:\n",
    "        if verbose:\n",
    "            print(\"Shape of input ids: \", input_ids.shape, input_ids.dtype)\n",
    "            print(\n",
    "                \"Shape of attention mask: \", attention_mask.shape, attention_mask.dtype\n",
    "            )\n",
    "            print(\"Shape of labels: \", labels.shape, labels.dtype)\n",
    "            print(f\"Using {device} device\")\n",
    "            print(model)\n",
    "        break\n",
    "    \n",
    "    tic = perf_counter()\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    corrects = []\n",
    "\n",
    "    for t in range(epochs):\n",
    "        if verbose:\n",
    "            print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loss = train(\n",
    "            train_dataloader, model, optimizer, device, step_size, verbose\n",
    "        )\n",
    "        test_loss, correct = test(test_dataloader, model, device, verbose)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        corrects.append(correct)\n",
    "        if file:\n",
    "            torch.save(model, f\"{file}-{t+1}.pth\")\n",
    "            if verbose:\n",
    "                print(f\"Saved PyTorch Model State to {file}-{t+1}.pth\")\n",
    "        if verbose:\n",
    "            toc = perf_counter()\n",
    "            print(f\"Done Epoch {t+1} in {toc - tic} seconds \\n\")\n",
    "    if verbose:\n",
    "        print(\"Done!\")\n",
    "\n",
    "    return train_losses, test_losses, corrects\n",
    "\n",
    "\n",
    "def get_augmented_sentences(\n",
    "    df,\n",
    "    col1,\n",
    "    col2,\n",
    "    alpha_sr=0.1,\n",
    "    alpha_ri=0.1,\n",
    "    alpha_rs=0.1,\n",
    "    p_rd=0.1,\n",
    "    num_aug=9,\n",
    "):\n",
    "    def augment(val1, val2):\n",
    "        return pd.Series(\n",
    "            eda(\n",
    "                val2,\n",
    "                alpha_sr,\n",
    "                alpha_ri,\n",
    "                alpha_rs,\n",
    "                p_rd,\n",
    "                num_aug,\n",
    "                stop_words + val1.split(),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    augmented_sentences = df.apply(lambda x: augment(x[col1], x[col2]), axis=1)\n",
    "    augmented_sentences = augmented_sentences.transpose()\n",
    "    augmented_sentences = augmented_sentences.rename(columns=df[col1].to_dict())\n",
    "    augmented_sentences = augmented_sentences.melt(var_name=col1, value_name=col2)\n",
    "    return augmented_sentences\n",
    "\n",
    "\n",
    "def get_augmented_labels(df, col1, col2, num_aug=9):\n",
    "    def augment(val1, val2):\n",
    "        if isin(val1, val2):\n",
    "            val3 = f\" {df[col1].sample().iloc[0]} \"\n",
    "            val2 = f\" {val2} \".replace(f\" {val1} \", val3).strip()\n",
    "            val1 = val3.strip()\n",
    "        return pd.Series([val1, val2], index=[col1, col2])\n",
    "\n",
    "    augmented_labels = pd.concat(\n",
    "        [df.apply(lambda x: augment(x[col1], x[col2]), axis=1) for _ in range(num_aug)],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    return augmented_labels\n",
    "\n",
    "\n",
    "def isin(value, sentence):\n",
    "    value = f\" {value} \"\n",
    "    sentence = f\" {sentence} \"\n",
    "    return value in sentence\n",
    "\n",
    "\n",
    "def startswith(sentence, value):\n",
    "    sentence = f\"{sentence} \"\n",
    "    value = f\"{value} \"\n",
    "    return sentence.startswith(value)\n",
    "\n",
    "\n",
    "def get_dataset(\n",
    "    tokenizer,\n",
    "    df,\n",
    "    col1,\n",
    "    col2,\n",
    "    task_prefix=\"\",\n",
    "    max_source_length=512,\n",
    "    max_target_length=128,\n",
    "):\n",
    "    def get_concatenation(val):\n",
    "        return task_prefix + val\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        df[col1].apply(get_concatenation).tolist(),\n",
    "        padding=\"longest\",\n",
    "        max_length=max_source_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids, attention_mask = encoding.input_ids, encoding.attention_mask\n",
    "\n",
    "    target_encoding = tokenizer(\n",
    "        df[col2].tolist(),\n",
    "        padding=\"longest\",\n",
    "        max_length=max_target_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "    labels = target_encoding.input_ids\n",
    "    \n",
    "    labels = torch.tensor(labels)\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "    dataset = SeqToSeqDataset(input_ids, attention_mask, labels)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10623592-d752-4b36-b52d-b37a75b6b435",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Obtaining Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930215bc-0599-430b-b1b4-b14d630332f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"../data/Hackathon_Ideal_Data.csv\")\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cc6c2c-b0cd-4e88-a5be-7fb9cf9e37dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scrubbing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5931f2-95ca-47b3-9dd4-69e5c52db904",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1[[\"MBRD\", \"BRD\"]]\n",
    "df2 = df2.rename(columns={\"MBRD\": \"brand\", \"BRD\": \"product\"})\n",
    "df2[\"brand\"] = df2[\"brand\"].apply(get_only_chars)\n",
    "df2[\"brand\"] = df2[\"brand\"].str.strip()\n",
    "df2 = df2[df2[\"brand\"].str.len() > 0]\n",
    "df2[\"product\"] = df2[\"product\"].apply(get_only_chars)\n",
    "df2[\"product\"] = df2[\"product\"].str.strip()\n",
    "df2 = df2[df2[\"product\"].str.len() > 0]\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31df32f7-394e-4611-b99e-e055dc32656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set1 = df2.sample(frac=0.7, random_state=1)\n",
    "training_set1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9165f4a9-a249-4696-8500-8dee0d2146d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = df2.drop(training_set1.index)\n",
    "validation_set = validation_set.sample(frac=0.7, random_state=1)\n",
    "validation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3184c96-559a-4469-9645-4739f3088022",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = df2.drop(training_set1.index)\n",
    "test_set = test_set.drop(validation_set.index)\n",
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cff0a2-f44f-4955-a135-b41c5b800f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1)\n",
    "augmented_sentences = get_augmented_sentences(\n",
    "    training_set1,\n",
    "    \"brand\",\n",
    "    \"product\",\n",
    "    alpha_sr=0.1,\n",
    "    alpha_ri=0.1,\n",
    "    alpha_rs=0.1,\n",
    "    p_rd=0.1,\n",
    "    num_aug=4,\n",
    ")\n",
    "training_set2 = pd.concat([training_set1, augmented_sentences])\n",
    "training_set2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bd1aae-e216-42f2-834b-0ed95048fc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_seed(1)\n",
    "augmented_labels = get_augmented_labels(training_set1, \"brand\", \"product\", num_aug=4)\n",
    "training_set3 = pd.concat([training_set1, augmented_labels])\n",
    "training_set3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3454c9b3-a58c-4612-a7fd-407bca296a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set4 = pd.concat([training_set2, augmented_labels])\n",
    "training_set4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7dcd88-dc9f-4485-a789-bb5b970eec5a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exploring Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f461b01e-2636-4fbe-a6df-cde5f8cafe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set1[\"brand\"].str.split(\" \").str.len().value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40a11fb-0850-4a6a-a1ac-127d6537074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set1.apply(\n",
    "    lambda x: isin(x[\"product\"], x[\"brand\"]), axis=1\n",
    ").value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c807ac-7cde-4b58-a378-03ee7582842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set1.apply(\n",
    "    lambda x: startswith(x[\"product\"], x[\"brand\"]), axis=1\n",
    ").value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3139e7a-5fb7-49b9-a934-05b625348e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set1.apply(\n",
    "    lambda x: isin(x[\"product\"], x[\"brand\"])\n",
    "    and not startswith(x[\"product\"], x[\"brand\"]),\n",
    "    axis=1,\n",
    ").value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce985a18-3786-416c-ba6c-0a19edc6ce75",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Modelling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f384fbfe-a92a-4dca-90b9-778b11f8366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = torch.load(r\"..\\models\\t5-small-tokenizer.pth\")\n",
    "model = torch.load(r\"..\\models\\t5-small-model.pth\").to(device)\n",
    "training_data1 = get_dataset(tokenizer, training_set1[\"document\"])\n",
    "training_data2 = get_dataset(tokenizer, training_set2[\"document\"])\n",
    "training_data3 = get_dataset(tokenizer, training_set3[\"document\"])\n",
    "training_data4 = get_dataset(tokenizer, training_set4[\"document\"])\n",
    "validation_data = get_dataset(tokenizer, validation_set, \"product\", \"brand\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "train_losses, test_losses, corrects = learn(\n",
    "    training_data,\n",
    "    validation_data,\n",
    "    model,\n",
    "    optimizer,\n",
    "    batch_size=64,\n",
    "    device=device,\n",
    "    epochs=387,\n",
    "    step_size=1,\n",
    "    file=r\"..\\models\\t5-small\",\n",
    "    verbose=verbose,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e573ad-8073-49ae-bfb6-01ac9c333222",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Interpreting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88fbed2-c3fd-4f05-a405-2db27000c471",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = torch.load(r\"D:\\models\\seq\\t5-small-tokenizer.pth\")\n",
    "model = torch.load(r\"D:\\brands\\t5-small-edabr-39.pth\").to(device)\n",
    "validation_data = get_dataset(tokenizer, validation_set, \"product\", \"brand\")\n",
    "validation_data = DataLoader(validation_data, batch_size=256)\n",
    "test_loss, correct = test(validation_data, model, device=device, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ee7943-3b35-4114-901a-be66f2c6f8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = test_set.shape[0]\n",
    "outputs = test_set[\"product\"].str.extract(\"^([^ ]*) ?.*$\", expand=False)\n",
    "correct = (outputs == test_set[\"brand\"]).sum()\n",
    "correct /= size\n",
    "print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aba62a8-bfcd-4df8-b431-8a742e51d0b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
